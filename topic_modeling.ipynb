{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132e56cf-175a-4b1c-a5cb-51c5868c0eea",
   "metadata": {},
   "source": [
    "# 1 Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757166d-cc88-4786-9c63-11678386c96c",
   "metadata": {},
   "source": [
    "## 1.1 Introduction\n",
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do `Latent Dirichlet Allocation` (LDA), which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide \n",
    "1) A document-term matrix and\n",
    "2) The number of topics you would like the algorithm to pick up. \n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad52034-aa10-4240-91ce-bf4537929371",
   "metadata": {},
   "source": [
    "## 1.2 Topic Modeling - Attempt: 1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330b2339-7c98-49e0-ace3-a7dfe4948dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abc</th>\n",
       "      <th>abducted</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abled</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abracadabra</th>\n",
       "      <th>abs</th>\n",
       "      <th>...</th>\n",
       "      <th>zam</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeroing</th>\n",
       "      <th>zeyde</th>\n",
       "      <th>zim</th>\n",
       "      <th>zipped</th>\n",
       "      <th>zippers</th>\n",
       "      <th>zookeeper</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tom</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ahir</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ari</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 6499 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ab  abc  abducted  abiding  ability  able  abled  abortion  \\\n",
       "tom       2    0         0        0        0     1      0         0   \n",
       "anthony   0    0         0        0        0     1      0         0   \n",
       "bill      0    0         0        0        0     0      1         2   \n",
       "ali       0    0         0        0        0     0      0         0   \n",
       "ahir      0    0         0        3        1     6      0         0   \n",
       "ari       0    0         0        0        0     2      0         0   \n",
       "gabriel   0    1         0        0        1     3      0         0   \n",
       "nate      0    0         0        0        1     0      0         0   \n",
       "joe       0    0         1        0        0     0      0         0   \n",
       "\n",
       "         abracadabra  abs  ...  zam  zero  zeroing  zeyde  zim  zipped  \\\n",
       "tom                0    7  ...    3     0        0      0    5       0   \n",
       "anthony            0    0  ...    0     1        0      0    0       0   \n",
       "bill               0    0  ...    0     0        0      0    0       0   \n",
       "ali                0    0  ...    0     0        1      0    0       0   \n",
       "ahir               0    0  ...    0     0        0      0    0       0   \n",
       "ari                0    0  ...    0     3        0      5    0       0   \n",
       "gabriel            0    0  ...    0     0        0      0    0       0   \n",
       "nate               0    0  ...    0     1        0      0    0       1   \n",
       "joe                1    0  ...    0     3        0      0    0       0   \n",
       "\n",
       "         zippers  zookeeper  zoom  zuko  \n",
       "tom            0          0     0     0  \n",
       "anthony        0          0     0     0  \n",
       "bill           0          0     0     0  \n",
       "ali            0          0     1     1  \n",
       "ahir           0          0     0     0  \n",
       "ari            0          0     0     0  \n",
       "gabriel        0          0     0     0  \n",
       "nate           1          1     0     0  \n",
       "joe            0          0     1     0  \n",
       "\n",
       "[9 rows x 6499 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the document term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "data = pd.read_pickle('data/dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4495910-4023-4ef8-aaa8-951a7a39fed9",
   "metadata": {},
   "source": [
    "Install the `gensim` library by running following command in terminal:\n",
    "```shell\n",
    "conda install -c conda-forge gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a25651-6a75-4670-932f-87f32b7e9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75980c69-7335-48cc-9408-d0ece72380a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d2a665-e59e-42e9-9256-bf5339e11d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "from scipy import sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02a504b-4b40-4b5d-bbf6-22279d8100d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tom</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>ali</th>\n",
       "      <th>ahir</th>\n",
       "      <th>ari</th>\n",
       "      <th>gabriel</th>\n",
       "      <th>nate</th>\n",
       "      <th>joe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abducted</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abiding</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tom  anthony  bill  ali  ahir  ari  gabriel  nate  joe\n",
       "ab          2        0     0    0     0    0        0     0    0\n",
       "abc         0        0     0    0     0    0        1     0    0\n",
       "abducted    0        0     0    0     0    0        0     0    1\n",
       "abiding     0        0     0    0     3    0        0     0    0\n",
       "ability     0        0     0    0     1    0        1     1    0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31537a5f-bb80-4e56-b893-44b321c2bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TDM to a CSR (Compressed Sparse Row) matrix\n",
    "sparse_counts = sparse.csr_matrix(tdm)\n",
    "\n",
    "# Convert the sparse matrix to Gensim corpus format\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27b503ab-f017-40cd-9719-e54ad3384d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cv.pkl', 'rb') as file:\n",
    "    cv = pickle.load(file)\n",
    "id2word = dict((v,k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a76bf-f1c7-43d9-9af6-81c1634dd1c6",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\\\n",
    "we need to specify two other parameters - the number of topics and the number of passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53356a70-571e-4eb5-b3f8-79201e8a0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for two number of topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbdd03bf-654b-4f9d-aca2-7db9c90e924c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"right\" + 0.010*\"people\" + 0.009*\"youre\" + 0.008*\"yeah\" + 0.006*\"gonna\" + 0.006*\"think\" + 0.005*\"uh\" + 0.005*\"theyre\" + 0.005*\"fucking\" + 0.005*\"say\"'),\n",
       " (1,\n",
       "  '0.022*\"laughing\" + 0.009*\"youre\" + 0.008*\"got\" + 0.007*\"think\" + 0.007*\"right\" + 0.007*\"gonna\" + 0.006*\"people\" + 0.006*\"time\" + 0.006*\"theyre\" + 0.005*\"cheering\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7590388-1a0d-469e-8dad-659a3875c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for three number of topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1e2e308-ebb2-4bd5-ab31-e29ef5497d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"youre\" + 0.001*\"people\" + 0.001*\"right\" + 0.001*\"yeah\" + 0.001*\"think\" + 0.000*\"gonna\" + 0.000*\"fucking\" + 0.000*\"time\" + 0.000*\"oh\" + 0.000*\"theyre\"'),\n",
       " (1,\n",
       "  '0.011*\"people\" + 0.011*\"right\" + 0.010*\"youre\" + 0.008*\"think\" + 0.007*\"fucking\" + 0.007*\"yeah\" + 0.006*\"gonna\" + 0.006*\"say\" + 0.005*\"theyre\" + 0.005*\"got\"'),\n",
       " (2,\n",
       "  '0.024*\"laughing\" + 0.010*\"right\" + 0.009*\"youre\" + 0.008*\"gonna\" + 0.008*\"got\" + 0.007*\"yeah\" + 0.007*\"want\" + 0.006*\"time\" + 0.006*\"think\" + 0.006*\"theyre\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22ef74b7-4872-458a-b79b-05029817298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for four number of topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43f2f6f0-ba36-47bc-aee2-344912895d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "008c0e07-4819-4fc3-9185-344c87ad41a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.035*\"laughing\" + 0.008*\"chuckles\" + 0.008*\"right\" + 0.008*\"youre\" + 0.007*\"gonna\" + 0.007*\"got\" + 0.007*\"time\" + 0.006*\"cheering\" + 0.005*\"oh\" + 0.005*\"yeah\"')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d25fb-c3bd-4bd3-a193-c26371ae5d56",
   "metadata": {},
   "source": [
    "we can try to modify the parameters to get better topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0158c-356b-4b2a-b95b-82628e48f3d0",
   "metadata": {},
   "source": [
    "## 1.3 Topic Modeling - Attempt: 2 (Nouns Only)\n",
    "One popular trick is to look only at terms that ar from one part of speech (only nouns, only adjectives, etc). Checkout the Upenn tag set: [Alphabetical list of part-of-speech tags used in the Penn Treebank Project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30a608-7391-4754-8785-9bfd272bb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    \"\"\"Given a string of text, tokenize the text and pull out only the nouns.\"\"\"\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [ word for (word, pos) in pos_tag(tokenized) if is_noun]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b13846-7192-4b49-9069-ce6870ce7473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data/data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8122801-130d-4204-85d1-effabc1034a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns \n",
    "from sklearn.feature extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "# Re-add the additional stop words since we are recreating the document-term matrix \n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people', 'youre',\n",
    "                  'got', 'gonna', 'time', 'think', 'yeah', 'said' ]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words) \n",
    "\n",
    "# Recreate a document-term matrix with only nouns \n",
    "cvn = CountVectorizer(stop_words=stop_words) \n",
    "data_cvn =  cvn.fit_transform(data_nouns.transcript) \n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=ovn.get_feature_names()) \n",
    "data_dlmn.index = data_nouns.index \n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e0d4f-07d1-45c6-afa8-5a7a1c86b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TDM to a CSR (Compressed Sparse Row) matrix\n",
    "# Convert the sparse matrix to Gensim corpus format\n",
    "corpusn = matutils.Sparse2Corpus(sparse.csr_matrix(data_dtmn.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85783940-01d4-473d-ac2c-477d8d5a4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2wordn = dict((v,k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723f754-b29a-4bba-a235-5f65181d7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for four number of topics\n",
    "ldan = models.LdaModel(corpus=corpusn, id2word=id2wordn, num_topics=2, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4288c-c82a-4ecc-958c-3a8c0fb959fd",
   "metadata": {},
   "source": [
    "## 1.4 Topic Modeling - Attempt:3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4780de-ae5c-4177-976f-8060e998f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    \"\"\"Given a string of text, tokenize the text and pull out only the nouns.\n",
    "    na: noun adjective\"\"\"\n",
    "    is_na = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nas = [ word for (word, pos) in pos_tag(tokenized) if is_na]\n",
    "    return ' '.join(all_nas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9f9ec-a286-425a-b5d8-e8857001a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nas = pd.DataFrame(data_clean.transcript.apply(nas))\n",
    "data_nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3634923-c3c1-43ac-98c8-911c959b4e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
